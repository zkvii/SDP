{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindarmour import MembershipInference\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindarmour.utils import LogUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = LogUtil.get_instance()\n",
    "TAG = \"MembershipInference_test\"\n",
    "LOGGER.set_level(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:32,500 - mindformers[auto_class.py:756] - INFO - default yaml config in ./checkpoint_download/gpt2/gpt2.yaml is used.\n",
      "2023-09-21 02:50:32,501 - mindformers[auto_class.py:661] - INFO - Config in the yaml file ./checkpoint_download/gpt2/gpt2.yaml are used for tokenizer building.\n",
      "2023-09-21 02:50:32,518 - mindformers[auto_class.py:668] - INFO - Load the tokenizer name GPT2Tokenizer from the ./checkpoint_download/gpt2/gpt2.yaml\n",
      "2023-09-21 02:50:32,519 - mindformers[base_tokenizer.py:2042] - INFO - Download the vocab from the url https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/gpt2/vocab.json to ./checkpoint_download/gpt2/vocab.json.\n",
      "2023-09-21 02:50:32,622 - mindformers[download_tools.py:81] - INFO - Start download ./checkpoint_download/gpt2/vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 1.04MB [00:00, 9.19MB/s]                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:32,739 - mindformers[download_tools.py:93] - INFO - Download completed!,times: 0.22s\n",
      "2023-09-21 02:50:32,742 - mindformers[base_tokenizer.py:2042] - INFO - Download the vocab from the url https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/XFormer_for_mindspore/gpt2/merges.txt to ./checkpoint_download/gpt2/merges.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:32,865 - mindformers[download_tools.py:81] - INFO - Start download ./checkpoint_download/gpt2/merges.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 457kB [00:00, 4.69MB/s]                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:32,967 - mindformers[download_tools.py:93] - INFO - Download completed!,times: 0.22s\n",
      "2023-09-21 02:50:32,990 - mindformers[base_tokenizer.py:1979] - INFO - config in the yaml file ./checkpoint_download/gpt2/gpt2.yaml are used for tokenizer building.\n",
      "2023-09-21 02:50:33,016 - mindformers[base_tokenizer.py:1988] - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-09-21 02:50:33,017 - mindformers[base_tokenizer.py:1994] - INFO - build tokenizer class name is: GPT2Tokenizer using args {'unk_token': '<|endoftext|>', 'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'vocab_file': './checkpoint_download/gpt2/vocab.json', 'merges_file': './checkpoint_download/gpt2/merges.txt'}.\n",
      "2023-09-21 02:50:33,066 - mindformers[auto_class.py:767] - INFO - GPT2Tokenizer Tokenizer built successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[WARNING] ME(1574942:140184214538048,MainProcess):2023-09-21-02:50:33.876.71 [mindspore/ops/primitive.py:228] The in_strategy of the operator in your network will not take effect in stand_alone mode. This means the the shard function called in the network is ignored. \n",
      "If you want to enable it, please use semi auto or auto parallel mode by context.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL or context.set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL)\n",
      "[WARNING] ME(1574942:140184214538048,MainProcess):2023-09-21-02:50:33.172.923 [mindspore/common/parameter.py:778] This interface may be deleted in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:33,680 - mindformers[download_tools.py:81] - INFO - Start download ./checkpoint_download/gpt2/gpt2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 498MB [00:21, 23.0MB/s]                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:55,290 - mindformers[download_tools.py:93] - INFO - Download completed!,times: 21.74s\n",
      "2023-09-21 02:50:55,294 - mindformers[base_model.py:117] - INFO - start to read the ckpt file: 497772028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-21 02:50:56,262 - mindformers[base_model.py:121] - INFO - weights in ./checkpoint_download/gpt2/gpt2.ckpt are loaded\n",
      "2023-09-21 02:50:56,267 - mindformers[auto_class.py:418] - INFO - model built successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mindformers import AutoModel, AutoTokenizer, TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mindspore import load_checkpoint,load_param_into_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_dict=load_checkpoint('./output/checkpoint/rank_0/gpt_rank_0-1170_1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_=load_param_into_net(model,params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "        {\n",
    "            \"method\": \"lr\",\n",
    "            \"params\": {\n",
    "                \"C\": np.logspace(-4, 2, 10)\n",
    "            }\n",
    "        },\n",
    "     {\n",
    "            \"method\": \"knn\",\n",
    "            \"params\": {\n",
    "                \"n_neighbors\": [3, 5, 7]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"mlp\",\n",
    "            \"params\": {\n",
    "                \"hidden_layer_sizes\": [(64,), (32, 32)],\n",
    "                \"solver\": [\"adam\"],\n",
    "                \"alpha\": [0.0001, 0.001, 0.01]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"rf\",\n",
    "            \"params\": {\n",
    "                \"n_estimators\": [100],\n",
    "                \"max_features\": [\"auto\", \"sqrt\"],\n",
    "                \"max_depth\": [5, 10, 20, None],\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4]\n",
    "            }\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"precision\", \"accuracy\", \"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common import initializer as init\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore.train import Model\n",
    "import mindspore.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "opt = nn.Momentum(params=model.trainable_params(), learning_rate=0.1, momentum=0.9,\n",
    "                  weight_decay=5e-4, loss_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=Model(network=model,loss_fn=loss,optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = MembershipInference(base_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
